# app.py
import streamlit as st
import requests
import json
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# -----------------------------------------------------------------
# [ì…€ 3: (ë‹¤ì¤‘ ê²€ìƒ‰) 'ë„êµ¬' ì •ì˜] - ìˆ˜ì •ëœ ìµœì¢… ë²„ì „
# -----------------------------------------------------------------
def search_precedents_by_keywords(keywords: list, max_per_keyword: int = 3) -> list:
    """
    í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ê° í‚¤ì›Œë“œë³„ë¡œ APIë¥¼ í˜¸ì¶œí•˜ì—¬
    'íŒë¡€ì¼ë ¨ë²ˆí˜¸'ì˜ 'ì¤‘ë³µ ì—†ëŠ”' ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    'íŒë¡€ëª…(1)'ê³¼ 'ë³¸ë¬¸(2)' ê²€ìƒ‰ì„ ëª¨ë‘ ìˆ˜í–‰.
    """
    print(f"Tool 1: search_precedents_by_keywords í˜¸ì¶œ (í‚¤ì›Œë“œ: {keywords})")
    
    # [Streamlit ìˆ˜ì • 1] st.secretsì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = st.secrets.get("LAW_API_KEY")
    if not api_key:
        print("Error: LAW_API_KEYê°€ secretsì— ì—†ìŠµë‹ˆë‹¤.")
        st.error("LAW_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    base_url = "http://www.law.go.kr/DRF/lawSearch.do"
    unique_ids = set()

    for keyword in keywords:
        for search_type in ["1", "2"]:
            search_type_name = "íŒë¡€ëª…" if search_type == "1" else "ë³¸ë¬¸"
            print(f"  -> í‚¤ì›Œë“œ '{keyword}' ({search_type_name} ê²€ìƒ‰) ì‹œë„...")
            
            params = {
                "OC": api_key, "target": "prec", "type": "JSON",
                "query": keyword, "search": search_type,
                "sort": "ddes", "display": max_per_keyword
            }
            try:
                response = requests.get(base_url, params=params)
                response.raise_for_status()
                data = response.json()
                precedents = data.get('PrecSearch', {}).get('prec', [])
                
                if precedents:
                    for prec in precedents:
                        pid = prec.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸')
                        if pid:
                            unique_ids.add(pid)
            except Exception as e:
                print(f"  -> í‚¤ì›Œë“œ '{keyword}' ({search_type_name} ê²€ìƒ‰) ì¤‘ ì˜¤ë¥˜: {e}")
                continue
            
    if not unique_ids:
        print("  -> 'íŒë¡€ ëª©ë¡' ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return []

    final_id_list = list(unique_ids)
    print(f"  -> 'íŒë¡€ ëª©ë¡' ì´ {len(final_id_list)}ê±´ì˜ ê³ ìœ  ID ì¶”ì¶œ ì„±ê³µ.")
    return final_id_list

def get_precedent_detail(precedent_id: str) -> dict:
    print(f"Tool 2: get_precedent_detail í˜¸ì¶œ (ID: {precedent_id})")
    
    # [Streamlit ìˆ˜ì • 2] st.secretsì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = st.secrets.get("LAW_API_KEY")
    if not api_key: return {}

    base_url = "http://www.law.go.kr/DRF/lawService.do"
    params = {"OC": api_key, "target": "prec", "ID": precedent_id, "type": "JSON"}
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        data = response.json()
        detail_data = data.get('PrecService', {})
        if not detail_data:
            print(f"  -> 'íŒë¡€ ë³¸ë¬¸' (ID: {precedent_id}) ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return {}
        print(f"  -> 'íŒë¡€ ë³¸ë¬¸' ê²€ìƒ‰ ì„±ê³µ: {detail_data.get('ì‚¬ê±´ëª…')}")
        return detail_data
    except Exception as e:
        print(f"Error in get_precedent_detail: {e}")
        return {}

# -----------------------------------------------------------------
# [ì…€ 4: (ì¢…í•© ì¶”ë¡ ) RAG íŒŒì´í”„ë¼ì¸ ì •ì˜]
# -----------------------------------------------------------------

# [Streamlit ìˆ˜ì • 3] st.secretsì—ì„œ OpenAI í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")

# 1. ìŸì  ë„ì¶œìš© LLM ì²´ì¸ (ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ 'ì˜ˆì‹œ 7ê°œ' ë²„ì „)
llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=OPENAI_API_KEY)
system_prompt_text = """
ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¼ìƒì ì¸ ì§ˆë¬¸ì„ ë²•ë¥  APIì˜ 'ë³¸ë¬¸' ë° 'íŒë¡€ëª…' ê²€ìƒ‰ì— ìµœì í™”ëœ 'ê²€ìƒ‰ìš© ë²•ë¥  í‚¤ì›Œë“œ' 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
í‚¤ì›Œë“œëŠ” ê°€ì¥ êµ¬ì²´ì ì¸ ìš©ì–´ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ ìš©ì–´ ìˆœì„œë¡œ ìƒì„±í•©ë‹ˆë‹¤.
í‚¤ì›Œë“œëŠ” ë²•ë¥ ì  ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ ë‹´ì•„ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 'ìˆ˜ë¦¬ì˜ë¬´' ëŒ€ì‹  'ì„ëŒ€ì¸ ìˆ˜ì„ ì˜ë¬´')
ì¶œë ¥ì€ ì˜¤ì§ 'í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3' í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ì–´ë– í•œ ì ‘ë‘ì‚¬ë‚˜ ë”°ì˜´í‘œë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
[ì˜ˆì‹œ 1]
ì§ˆë¬¸: "ì•Œë°” ì›”ê¸‰ì„ ëª» ë°›ì•˜ì–´ìš”."
ì¶œë ¥: ì„ê¸ˆì²´ë¶ˆ, ê·¼ë¡œê¸°ì¤€ë²•ìœ„ë°˜, ì„ê¸ˆ
[ì˜ˆì‹œ 2]
ì§ˆë¬¸: "ì¤‘ê³ ê±°ë˜ ë²½ëŒ ë°°ì†¡"
ì¶œë ¥: ì¤‘ê³ ê±°ë˜ ì‚¬ê¸°, ì±„ë¬´ë¶ˆì´í–‰, ì†í•´ë°°ìƒ
[ì˜ˆì‹œ 3]
ì§ˆë¬¸: "ì‚¬ì§„ ë„ìš©"
ì¶œë ¥: ì €ì‘ê¶Œ ì¹¨í•´, ì†í•´ë°°ìƒ(ì§€), ì´ˆìƒê¶Œ
[ì˜ˆì‹œ 4]
ì§ˆë¬¸: "ìœ—ì§‘ì´ ë„ˆë¬´ ì‹œë„ëŸ¬ì›Œìš”"
ì¶œë ¥: ì¸µê°„ì†ŒìŒ ì†í•´ë°°ìƒ, ì¸ê²©ê¶Œ ì¹¨í•´, ìœ„ìë£Œ
[ì˜ˆì‹œ 5]
ì§ˆë¬¸: "ê¸¸ ê°€ë‹¤ê°€ ì˜† ê±´ë¬¼ì—ì„œ ë–¨ì–´ì§„ ê°„íŒì— ë§ì•„ì„œ ë‹¤ì³¤ì–´ìš”."
ì¶œë ¥: ê³µì‘ë¬¼ì±…ì„, ì†í•´ë°°ìƒ(ê¸°), ì•ˆì „ì˜ë¬´ìœ„ë°˜
[ì˜ˆì‹œ 6]
ì§ˆë¬¸: "ì›”ì„¸ì§‘ ë³´ì¼ëŸ¬ê°€ ê³ ì¥ë‚¬ëŠ”ë° ì§‘ì£¼ì¸ì´ ìˆ˜ë¦¬ë¥¼ ì•ˆ í•´ì¤˜ìš”."
ì¶œë ¥: ì„ëŒ€ì¸ ìˆ˜ì„ ì˜ë¬´, ëª©ì ë¬¼ ìˆ˜ë¦¬ì˜ë¬´, ì„ëŒ€ì°¨ê³„ì•½
[ì˜ˆì‹œ 7]
ì§ˆë¬¸: "ì•„ë²„ì§€ê°€ ëŒì•„ê°€ì…¨ëŠ”ë° ë¹šì´ ë” ë§ì•„ìš”."
ì¶œë ¥: ìƒì†í¬ê¸°, í•œì •ìŠ¹ì¸, ìƒì†ì±„ë¬´
"""
issue_extraction_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt_text),
    ("human", "ì§ˆë¬¸: {question}\nì¶œë ¥:"),
])
extract_issue_chain = issue_extraction_prompt | llm | StrOutputParser() | (lambda x: [k.strip() for k in x.split(',') if k.strip()])

# 2. RAG ì»´í¬ë„ŒíŠ¸
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

# 3. ìµœì¢… 'ì¢…í•© ì¶”ë¡ 'ìš© LLM ì²´ì¸ (ğŸŒŸ ë²„ê·¸ ìˆ˜ì •ë³¸ ğŸŒŸ)
final_reasoning_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë§¤ìš° ìœ ëŠ¥í•œ ëŒ€í•œë¯¼êµ­ ë³€í˜¸ì‚¬ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ APIë¡œ ê²€ìƒ‰ëœ 'ì—¬ëŸ¬ ê°œì˜ íŒë¡€ ìš”ì•½'ì…ë‹ˆë‹¤.
ì´ íŒë¡€ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì‚¬ìš©ìì—ê²Œ ë²•ë¥  ì¡°ì–¸ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]:
{question}

[ì°¸ê³  íŒë¡€ ëª©ë¡ (ìµœëŒ€ 3ê°œ)]:
{context}

[ë²•ë¥  ìë¬¸ (ì•„ë˜ ì–‘ì‹ ì¤€ìˆ˜)]:
1.  **í•µì‹¬ ìŸì :** (ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë²•ë¥  ìŸì ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

2.  **ê´€ë ¨ íŒë¡€ ë¶„ì„:** (ê²€ìƒ‰ëœ [ì°¸ê³  íŒë¡€ ëª©ë¡]ì´ ì´ ìŸì ê³¼ ì–´ë–»ê²Œ ê´€ë ¨ë˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤. 
    
    ğŸ’¡ **[ì¤‘ìš” ì§€ì‹œ]** íŒë¡€ë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” "[íŒë¡€ 1: {{ì‚¬ê±´ëª…}} ({{ì‚¬ê±´ë²ˆí˜¸}})]" í˜•ì‹ì—ì„œ **ë°˜ë“œì‹œ 'ì‚¬ê±´ë²ˆí˜¸'(ì˜ˆ: "2021ë„3451")ë¥¼ í•¨ê»˜ ì¸ìš©**í•˜ì„¸ìš”.
    
    [ì˜ˆì‹œ]
    * "ëŒ€ë²•ì› 2021ë„3451 íŒê²°(ì‚¬ê±´ëª…: ê°•ì œì¶”í–‰)ì—ì„œëŠ”..."
    * "ì°¸ê³  íŒë¡€(ì‚¬ê±´ë²ˆí˜¸: 2017ë‹¤12345)ì— ë”°ë¥´ë©´...")

3.  **ì¢…í•© ì¡°ì–¸ ë° ê²°ë¡ :** (ìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì—ê²Œ "ì–´ë–»ê²Œ í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•˜ë‹¤" ë˜ëŠ” "ì–´ë–¤ ì ì„ ì£¼ì¥í•  ìˆ˜ ìˆë‹¤"ëŠ” ì‹ì˜ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì œê³µ)
""")
reasoning_chain = final_reasoning_prompt | llm | StrOutputParser()

# 4. Document ë³€í™˜ í•¨ìˆ˜
def create_documents_and_format(details: list) -> list:
    """
    [Streamlit ìˆ˜ì •]
    íŒë¡€ ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸(details)ë¥¼ ë°›ì•„ ë²¡í„°í™”ë¥¼ ìœ„í•œ 'Document' ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜
    """
    documents = []
    
    for i, detail in enumerate(details):
        if not detail: continue
            
        # ë²¡í„°í™”(ìœ ì‚¬ë„ ê²€ìƒ‰)ì— ì‚¬ìš©í•  ë‚´ìš© (íŒì‹œì‚¬í•­ + íŒê²°ìš”ì§€)
        content_to_embed = (
            f"íŒì‹œì‚¬í•­: {detail.get('íŒì‹œì‚¬í•­', '')}\n\n"
            f"íŒê²°ìš”ì§€: {detail.get('íŒê²°ìš”ì§€', '')}"
        )
        
        # ğŸ’¡ [ìˆ˜ì • ì™„ë£Œ]
        # ì—ëŸ¬ê°€ ë°œìƒí•œ ... ë¶€ë¶„ì„ ì „ì²´ ì½”ë“œë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤.
        # ì´ metadataëŠ” ë‚˜ì¤‘ì— 5ë‹¨ê³„(ìœ ì‚¬ë„ ê²€ìƒ‰)ì—ì„œ ì²­í¬(chunk)ì™€ í•¨ê»˜ ì‚¬ìš©ë©ë‹ˆë‹¤.
        metadata = {
            "source_id": detail.get('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸', 'N/A'),
            "ì‚¬ê±´ëª…": detail.get('ì‚¬ê±´ëª…', 'N/A'),
            "ì‚¬ê±´ë²ˆí˜¸": detail.get('ì‚¬ê±´ë²ˆí˜¸', 'N/A'),
            "ì„ ê³ ì¼ì": detail.get('ì„ ê³ ì¼ì', 'N/A'),
            "ë²•ì›ëª…": detail.get('ë²•ì›ëª…', 'N/A'),
            # [ì°¸ê³ ] 'íŒë¡€ìƒì„¸ë§í¬'ëŠ” ë³¸ë¬¸ API(detail)ì— ì›ë˜ ì—†ìŠµë‹ˆë‹¤.
            # ë”°ë¼ì„œ ì´ ë§í¬ëŠ” í•­ìƒ '#'ìœ¼ë¡œ ì²˜ë¦¬ë˜ë©°, ì´ëŠ” ì •ìƒì…ë‹ˆë‹¤.
            "ìƒì„¸ë§í¬": f"http://www.law.go.kr{detail.get('íŒë¡€ìƒì„¸ë§í¬', '')}" if detail.get('íŒë¡€ìƒì„¸ë§í¬') else "#"
        }
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ (50ì ë¯¸ë§Œ) ìœ íš¨í•˜ì§€ ì•Šì€ íŒë¡€ë¡œ ê°„ì£¼
        if len(content_to_embed) < 50: continue
            
        # 1. ë²¡í„°í™”(ìœ ì‚¬ë„ ê²€ìƒ‰)ë¥¼ ìœ„í•œ Document ê°ì²´ ìƒì„±
        documents.append(Document(page_content=content_to_embed, metadata=metadata))
        
    print(f"  -> ì´ {len(details)}ê°œì˜ ë³¸ë¬¸ ì¤‘ {len(documents)}ê°œì˜ ìœ íš¨í•œ Document ìƒì„± ì™„ë£Œ.")
    return documents # ğŸŒŸ Document ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜


# -----------------------------------------------------------------
# [ì…€ 5: (ì¢…í•© ì¶”ë¡ ) ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰]
# -----------------------------------------------------------------

def run_legal_pipeline_reasoning(user_question: str) -> str:
    """
    [Streamlit ìˆ˜ì • 4] print() ëŒ€ì‹  ìµœì¢… ë‹µë³€ ë¬¸ìì—´ì„ return í•˜ë„ë¡ ìˆ˜ì •
    """
    
    print(f"--- [ì§ˆë¬¸: {user_question}] ---")
    
    # 1ë‹¨ê³„: ìŸì  ë„ì¶œ
    issue_keywords = extract_issue_chain.invoke({"question": user_question})
    print(f"ğŸ¤– AIê°€ ë„ì¶œí•œ ìŸì  ë¦¬ìŠ¤íŠ¸: {issue_keywords}")
    
    # 2ë‹¨ê³„: íŒë¡€ ëª©ë¡ ê²€ìƒ‰
    precedent_ids = search_precedents_by_keywords(issue_keywords, max_per_keyword=3)
    if not precedent_ids:
        return "[ìµœì¢… ë‹µë³€]\n- ì´ ìŸì ë“¤ë¡œ ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3ë‹¨ê³„: ê° íŒë¡€ ë³¸ë¬¸ ì¡°íšŒ
    precedent_details = []
    for pid in precedent_ids:
        detail = get_precedent_detail(pid)
        if detail:
            precedent_details.append(detail)
            
    if not precedent_details:
        return "[ìµœì¢… ë‹µë³€]\n- íŒë¡€ ëª©ë¡ì€ ì°¾ì•˜ìœ¼ë‚˜, ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    # 4ë‹¨ê³„: ë³¸ë¬¸ -> Document ë³€í™˜
    documents = create_documents_and_format(precedent_details)
    if not documents:
        return "[ìµœì¢… ë‹µë³€]\n- ìœ íš¨í•œ íŒë¡€ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    chunks = text_splitter.split_documents(documents)
    if not chunks:
        print("  -> ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ì²­í¬ ìƒì„± ì‹¤íŒ¨.")
        return "[ìµœì¢… ë‹µë³€]\n- ê²€ìƒ‰ëœ íŒë¡€ì˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    vectorstore = Chroma.from_documents(chunks, embeddings)
    
    # 5ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ìƒ‰ (Re-ranking)
    print(f"  -> ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘ (ì§ˆë¬¸: {user_question})")
    try:
        similar_chunks = vectorstore.similarity_search(user_question, k=3)
        if not similar_chunks:
            return "[ìµœì¢… ë‹µë³€]\n- ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        context_for_reasoning = ""
        seen_ids = set()
        for i, chunk in enumerate(similar_chunks):
            metadata = chunk.metadata
            source_id = metadata.get('source_id')
            if source_id in seen_ids:
                continue
            seen_ids.add(source_id)
            
            print(f"  -> ê°€ì¥ ìœ ì‚¬í•œ íŒë¡€ TOP {i+1} ì°¾ìŒ: {metadata.get('ì‚¬ê±´ëª…')}")
            context_for_reasoning += (
                f"[íŒë¡€ {i+1}: {metadata.get('ì‚¬ê±´ëª…')} ({metadata.get('ì‚¬ê±´ë²ˆí˜¸')})]\n"
                f"- ì„ ê³ ì¼ì/ë²•ì›: {metadata.get('ì„ ê³ ì¼ì')} / {metadata.get('ë²•ì›ëª…')}\n"
                f"- íŒì‹œ/ìš”ì§€: {chunk.page_content}\n"
                f"- [íŒë¡€ ì›ë¬¸ ë³´ê¸°]({metadata.get('ìƒì„¸ë§í¬')})\n\n"
            )

    except Exception as e:
        print(f"  -> ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return f"[ìµœì¢… ë‹µë³€]\n- ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    if not context_for_reasoning:
        return "[ìµœì¢… ë‹µë³€]\n- ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ)"

    # 6ë‹¨ê³„: ìµœì¢… 'ì¢…í•© ì¶”ë¡ ' (LLM)
    final_answer = reasoning_chain.invoke({
        "question": user_question,
        "context": context_for_reasoning
    })

    # 7. ìµœì¢… ë‹µë³€ ë¬¸ìì—´ êµ¬ì„±
    final_response_str = (
        f"**ğŸ¤– AIê°€ ë¶„ì„í•œ ë²•ë¥  ìŸì ì€ {issue_keywords}ì…ë‹ˆë‹¤.**\n\n"
        "ì´ ìŸì ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ëœ íŒë¡€ ì¤‘, ì‚¬ìš©ìë‹˜ì˜ ì§ˆë¬¸ê³¼ **ê°€ì¥ ìœ ì‚¬í•œ íŒë¡€ë“¤**ì„ ì¢…í•©í•˜ì—¬ ì¡°ì–¸í•´ ë“œë¦½ë‹ˆë‹¤.\n\n"
        f"{final_answer}"
    )
    
    return final_response_str

# -----------------------------------------------------------------
# [Streamlit UI ë¶€ë¶„]
# -----------------------------------------------------------------

st.title("âš–ï¸ AI ë²•ë¥  ìë¬¸ ì±—ë´‡")
st.write("ê¶ê¸ˆí•œ ë²•ë¥  ë¬¸ì œë¥¼ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. AIê°€ ê´€ë ¨ íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

# (ì„ íƒ) ì´ì „ ì§ˆë¬¸ ê¸°ë¡ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
if user_question := st.chat_input("ì›”ì„¸ì§‘ ë³´ì¼ëŸ¬ê°€ ê³ ì¥ë‚¬ëŠ”ë° ìˆ˜ë¦¬ë¥¼ ì•ˆ í•´ì¤˜ìš”..."):
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    # 2. AI ë‹µë³€ ì²˜ë¦¬
    with st.chat_message("assistant"):
        with st.spinner("AIê°€ íŒë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            response = run_legal_pipeline_reasoning(user_question)
            
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
